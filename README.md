# ML Incidents Prediction with XGBoost

This project demonstrates how to predict the number of incidents at a facility using machine learning techniques, specifically the XGBoost algorithm. The analysis covers data preprocessing, feature engineering, model training, hyperparameter tuning, and evaluation.

## Overview

The goal of this project is to forecast the number of incidents based on historical data. The primary steps include:

1. **Data Loading and Cleaning**: Read and preprocess incident data.
2. **Feature Engineering**: Extract and create relevant features from date columns.
3. **Model Building**: Train an XGBoost regressor model to predict the number of incidents.
4. **Evaluation**: Assess model performance using various metrics and visualize the results.
5. **Prediction**: Generate and export predictions.

## Dependencies

Ensure you have the following Python packages installed:

```bash
pip install pyarrow numpy pandas matplotlib seaborn plotly statsmodels scikit-learn xgboost eli5
```

## File Descriptions

- **ml-incidents-prediction-with-xgboost (3) (1).ipynb**: Jupyter notebook containing the complete pipeline for predicting the number of incidents. It includes data loading, preprocessing, feature engineering, model training, evaluation, and predictions.
  
- **Incident_Report.parquet**: Sample data file containing incident reports used for predictions.

- **XGBoost Predictions.xlsx**: Excel file containing the predictions generated by the XGBoost model.

- **XGBoost Incident Level Predictions.xlsx**: Excel file with predictions on incident levels.

## Usage

1. **Loading the Data**: 
   ```python
   repo_inci = pd.read_parquet('Incident_Report.parquet', engine='pyarrow')
   ```

2. **Preprocessing**: 
   - Extract features from the `DateOccured` column.
   - Handle missing values and outliers.
   - Scale the data.

3. **Model Training**: 
   - Train an `XGBRegressor` using the cleaned and scaled data.
   - Perform hyperparameter tuning with `GridSearchCV`.

4. **Evaluation**:
   - Evaluate the model using metrics like Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and R-Squared (R²).
   - Visualize predictions and compare with actual values.

5. **Export Predictions**: 
   - Save predictions to Excel files for further analysis.

## Example Code

Here is a snippet of code demonstrating how to fit the XGBoost model and make predictions:

```python
import pandas as pd
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load and preprocess data
repo_inci = pd.read_parquet('Incident_Report.parquet', engine='pyarrow')
# (Preprocessing steps here...)

# Prepare features and target variable
X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.1, shuffle=False)

# Train model
xgb = XGBRegressor()
xgb.fit(X_train, y_train)

# Predict and evaluate
xgb_pred = xgb.predict(X_valid)
print(f'MAE: {mean_absolute_error(y_valid, xgb_pred)}')
print(f'MSE: {mean_squared_error(y_valid, xgb_pred)}')
print(f'RMSE: {np.sqrt(mean_squared_error(y_valid, xgb_pred))}')
print(f'R²: {r2_score(y_valid, xgb_pred)}')

# Export predictions
valid['Predictions'] = xgb_pred
valid.to_excel('XGBoost Predictions.xlsx', index=False)
```

## Visualizations

- **Incidents Through Time**: Line plot showing the number of incidents over time.
- **Feature Importances**: Bar chart of feature importances according to the XGBoost model.

## Future Work

- Explore additional features and hyperparameter settings.
- Implement and compare different machine learning models.
- Conduct deeper analysis of prediction errors and refine the model.
